diff --git a/arch/x86/crypto/serpent-avx-x86_64-asm_64.S b/arch/x86/crypto/serpent-avx-x86_64-asm_64.S
index 8be5718..c5a9956 100644
--- a/arch/x86/crypto/serpent-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/serpent-avx-x86_64-asm_64.S
@@ -25,6 +25,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 #include "glue_helper-asm-avx.S"
 
 .file "serpent-avx-x86_64-asm_64.S"
@@ -619,7 +620,7 @@ __serpent_enc_blk8_avx:
 	write_blocks(RA1, RB1, RC1, RD1, RK0, RK1, RK2);
 	write_blocks(RA2, RB2, RC2, RD2, RK0, RK1, RK2);
 
-	ret;
+	pax_ret __serpent_enc_blk8_avx;
 ENDPROC(__serpent_enc_blk8_avx)
 
 .align 8
@@ -673,10 +674,10 @@ __serpent_dec_blk8_avx:
 	write_blocks(RC1, RD1, RB1, RE1, RK0, RK1, RK2);
 	write_blocks(RC2, RD2, RB2, RE2, RK0, RK1, RK2);
 
-	ret;
+	pax_ret __serpent_dec_blk8_avx;
 ENDPROC(__serpent_dec_blk8_avx)
 
-ENTRY(serpent_ecb_enc_8way_avx)
+RAP_ENTRY(serpent_ecb_enc_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -686,15 +687,15 @@ ENTRY(serpent_ecb_enc_8way_avx)
 
 	load_8way(%rdx, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
-	call __serpent_enc_blk8_avx;
+	pax_direct_call __serpent_enc_blk8_avx;
 
 	store_8way(%rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	FRAME_END
-	ret;
+	pax_ret serpent_ecb_enc_8way_avx;
 ENDPROC(serpent_ecb_enc_8way_avx)
 
-ENTRY(serpent_ecb_dec_8way_avx)
+RAP_ENTRY(serpent_ecb_dec_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -704,15 +705,15 @@ ENTRY(serpent_ecb_dec_8way_avx)
 
 	load_8way(%rdx, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
-	call __serpent_dec_blk8_avx;
+	pax_direct_call __serpent_dec_blk8_avx;
 
 	store_8way(%rsi, RC1, RD1, RB1, RE1, RC2, RD2, RB2, RE2);
 
 	FRAME_END
-	ret;
+	pax_ret serpent_ecb_dec_8way_avx;
 ENDPROC(serpent_ecb_dec_8way_avx)
 
-ENTRY(serpent_cbc_dec_8way_avx)
+RAP_ENTRY(serpent_cbc_dec_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -722,15 +723,15 @@ ENTRY(serpent_cbc_dec_8way_avx)
 
 	load_8way(%rdx, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
-	call __serpent_dec_blk8_avx;
+	pax_direct_call __serpent_dec_blk8_avx;
 
 	store_cbc_8way(%rdx, %rsi, RC1, RD1, RB1, RE1, RC2, RD2, RB2, RE2);
 
 	FRAME_END
-	ret;
+	pax_ret serpent_cbc_dec_8way_avx;
 ENDPROC(serpent_cbc_dec_8way_avx)
 
-ENTRY(serpent_ctr_8way_avx)
+RAP_ENTRY(serpent_ctr_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -742,15 +743,15 @@ ENTRY(serpent_ctr_8way_avx)
 	load_ctr_8way(%rcx, .Lbswap128_mask, RA1, RB1, RC1, RD1, RA2, RB2, RC2,
 		      RD2, RK0, RK1, RK2);
 
-	call __serpent_enc_blk8_avx;
+	pax_direct_call __serpent_enc_blk8_avx;
 
 	store_ctr_8way(%rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	FRAME_END
-	ret;
+	pax_ret serpent_ctr_8way_avx;
 ENDPROC(serpent_ctr_8way_avx)
 
-ENTRY(serpent_xts_enc_8way_avx)
+RAP_ENTRY(serpent_xts_enc_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -763,16 +764,16 @@ ENTRY(serpent_xts_enc_8way_avx)
 	load_xts_8way(%rcx, %rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2,
 		      RK0, RK1, RK2, .Lxts_gf128mul_and_shl1_mask);
 
-	call __serpent_enc_blk8_avx;
+	pax_direct_call __serpent_enc_blk8_avx;
 
 	/* dst <= regs xor IVs(in dst) */
 	store_xts_8way(%rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	FRAME_END
-	ret;
+	pax_ret serpent_xts_enc_8way_avx;
 ENDPROC(serpent_xts_enc_8way_avx)
 
-ENTRY(serpent_xts_dec_8way_avx)
+RAP_ENTRY(serpent_xts_dec_8way_avx)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -785,11 +786,11 @@ ENTRY(serpent_xts_dec_8way_avx)
 	load_xts_8way(%rcx, %rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2,
 		      RK0, RK1, RK2, .Lxts_gf128mul_and_shl1_mask);
 
-	call __serpent_dec_blk8_avx;
+	pax_direct_call __serpent_dec_blk8_avx;
 
 	/* dst <= regs xor IVs(in dst) */
 	store_xts_8way(%rsi, RC1, RD1, RB1, RE1, RC2, RD2, RB2, RE2);
 
 	FRAME_END
-	ret;
+	pax_ret serpent_xts_dec_8way_avx;
 ENDPROC(serpent_xts_dec_8way_avx)

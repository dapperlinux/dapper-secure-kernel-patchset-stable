diff --git a/arch/x86/lib/atomic64_cx8_32.S b/arch/x86/lib/atomic64_cx8_32.S
index db3ae854..5e266ef 100644
--- a/arch/x86/lib/atomic64_cx8_32.S
+++ b/arch/x86/lib/atomic64_cx8_32.S
@@ -10,6 +10,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/irq_vectors.h>
 #include <asm/alternative-asm.h>
 
 .macro read64 reg
@@ -22,9 +23,14 @@
 
 ENTRY(atomic64_read_cx8)
 	read64 %ecx
-	ret
+	pax_ret atomic64_read
 ENDPROC(atomic64_read_cx8)
 
+ENTRY(atomic64_read_unchecked_cx8)
+	read64 %ecx
+	pax_ret atomic64_read_unchecked
+ENDPROC(atomic64_read_unchecked_cx8)
+
 ENTRY(atomic64_set_cx8)
 1:
 /* we don't need LOCK_PREFIX since aligned 64-bit writes
@@ -32,20 +38,30 @@ ENTRY(atomic64_set_cx8)
 	cmpxchg8b (%esi)
 	jne 1b
 
-	ret
+	pax_ret atomic64_set
 ENDPROC(atomic64_set_cx8)
 
+ENTRY(atomic64_set_unchecked_cx8)
+1:
+/* we don't need LOCK_PREFIX since aligned 64-bit writes
+ * are atomic on 586 and newer */
+	cmpxchg8b (%esi)
+	jne 1b
+
+	pax_ret atomic64_set_unchecked
+ENDPROC(atomic64_set_unchecked_cx8)
+
 ENTRY(atomic64_xchg_cx8)
 1:
 	LOCK_PREFIX
 	cmpxchg8b (%esi)
 	jne 1b
 
-	ret
+	pax_ret atomic64_xchg
 ENDPROC(atomic64_xchg_cx8)
 
-.macro addsub_return func ins insc
-ENTRY(atomic64_\func\()_return_cx8)
+.macro addsub_return func ins insc unchecked=""
+ENTRY(atomic64_\func\()_return\unchecked\()_cx8)
 	pushl %ebp
 	pushl %ebx
 	pushl %esi
@@ -61,26 +77,36 @@ ENTRY(atomic64_\func\()_return_cx8)
 	movl %edx, %ecx
 	\ins\()l %esi, %ebx
 	\insc\()l %edi, %ecx
+
+.ifb \unchecked
+.if \func == add
+	PAX_REFCOUNT64_OVERFLOW (%ebp)
+.else
+	PAX_REFCOUNT64_UNDERFLOW (%ebp)
+.endif
+.endif
+
 	LOCK_PREFIX
 	cmpxchg8b (%ebp)
 	jne 1b
-
-10:
 	movl %ebx, %eax
 	movl %ecx, %edx
+
 	popl %edi
 	popl %esi
 	popl %ebx
 	popl %ebp
-	ret
-ENDPROC(atomic64_\func\()_return_cx8)
+	pax_ret atomic64_\func\()_return\unchecked
+ENDPROC(atomic64_\func\()_return\unchecked\()_cx8)
 .endm
 
 addsub_return add add adc
 addsub_return sub sub sbb
+addsub_return add add adc _unchecked
+addsub_return sub sub sbb _unchecked
 
-.macro incdec_return func ins insc
-ENTRY(atomic64_\func\()_return_cx8)
+.macro incdec_return func ins insc unchecked=""
+ENTRY(atomic64_\func\()_return\unchecked\()_cx8)
 	pushl %ebx
 
 	read64 %esi
@@ -89,20 +115,30 @@ ENTRY(atomic64_\func\()_return_cx8)
 	movl %edx, %ecx
 	\ins\()l $1, %ebx
 	\insc\()l $0, %ecx
+
+.ifb \unchecked
+.if \func == inc
+	PAX_REFCOUNT64_OVERFLOW (%esi)
+.else
+	PAX_REFCOUNT64_UNDERFLOW (%esi)
+.endif
+.endif
+
 	LOCK_PREFIX
 	cmpxchg8b (%esi)
 	jne 1b
-
-10:
 	movl %ebx, %eax
 	movl %ecx, %edx
+
 	popl %ebx
-	ret
-ENDPROC(atomic64_\func\()_return_cx8)
+	pax_ret atomic64_\func\()_return\unchecked
+ENDPROC(atomic64_\func\()_return\unchecked\()_cx8)
 .endm
 
 incdec_return inc add adc
 incdec_return dec sub sbb
+incdec_return inc add adc _unchecked
+incdec_return dec sub sbb _unchecked
 
 ENTRY(atomic64_dec_if_positive_cx8)
 	pushl %ebx
@@ -113,6 +149,9 @@ ENTRY(atomic64_dec_if_positive_cx8)
 	movl %edx, %ecx
 	subl $1, %ebx
 	sbb $0, %ecx
+
+	PAX_REFCOUNT64_UNDERFLOW (%esi)
+
 	js 2f
 	LOCK_PREFIX
 	cmpxchg8b (%esi)
@@ -122,7 +161,7 @@ ENTRY(atomic64_dec_if_positive_cx8)
 	movl %ebx, %eax
 	movl %ecx, %edx
 	popl %ebx
-	ret
+	pax_ret atomic64_dec_if_positive
 ENDPROC(atomic64_dec_if_positive_cx8)
 
 ENTRY(atomic64_add_unless_cx8)
@@ -144,6 +183,9 @@ ENTRY(atomic64_add_unless_cx8)
 	movl %edx, %ecx
 	addl %ebp, %ebx
 	adcl %edi, %ecx
+
+	PAX_REFCOUNT64_OVERFLOW (%esi)
+
 	LOCK_PREFIX
 	cmpxchg8b (%esi)
 	jne 1b
@@ -153,7 +195,7 @@ ENTRY(atomic64_add_unless_cx8)
 	addl $8, %esp
 	popl %ebx
 	popl %ebp
-	ret
+	pax_ret atomic64_add_unless
 4:
 	cmpl %edx, 4(%esp)
 	jne 2b
@@ -173,6 +215,9 @@ ENTRY(atomic64_inc_not_zero_cx8)
 	xorl %ecx, %ecx
 	addl $1, %ebx
 	adcl %edx, %ecx
+
+	PAX_REFCOUNT64_OVERFLOW (%esi)
+
 	LOCK_PREFIX
 	cmpxchg8b (%esi)
 	jne 1b
@@ -180,5 +225,5 @@ ENTRY(atomic64_inc_not_zero_cx8)
 	movl $1, %eax
 3:
 	popl %ebx
-	ret
+	pax_ret atomic64_inc_not_zero
 ENDPROC(atomic64_inc_not_zero_cx8)

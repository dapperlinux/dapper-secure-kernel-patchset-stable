diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
index ef766a3..d3f0e59 100644
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -37,6 +37,9 @@
 #include <asm/pgtable_types.h>
 #include <asm/export.h>
 #include <linux/err.h>
+#include <asm/pgtable.h>
+#include <asm/alternative-asm.h>
+#include <asm/current.h>
 
 /* Avoid __ASSEMBLER__'ifying <linux/audit.h> just for this.  */
 #include <linux/elf-em.h>
@@ -54,6 +57,392 @@ ENTRY(native_usergs_sysret64)
 ENDPROC(native_usergs_sysret64)
 #endif /* CONFIG_PARAVIRT */
 
+	.macro ljmpq sel, off
+#if defined(CONFIG_MPSC) || defined(CONFIG_MCORE2) || defined (CONFIG_MATOM)
+	.byte 0x48; ljmp *1234f(%rip)
+	.pushsection .rodata
+	.align 16
+	1234: .quad \off; .word \sel
+	.popsection
+#else
+	pushq	$\sel
+	pushq	$\off
+	lretq
+#endif
+	.endm
+
+	.macro pax_enter_kernel
+	pax_set_fptr_mask
+#if defined(CONFIG_PAX_KERNEXEC) || defined(CONFIG_PAX_MEMORY_UDEREF)
+	pax_direct_call pax_enter_kernel
+#endif
+	.endm
+
+	.macro pax_exit_kernel
+#if defined(CONFIG_PAX_KERNEXEC) || defined(CONFIG_PAX_MEMORY_UDEREF)
+	pax_direct_call pax_exit_kernel
+#endif
+	.endm
+
+#if defined(CONFIG_PAX_KERNEXEC) || defined(CONFIG_PAX_MEMORY_UDEREF)
+ENTRY(pax_enter_kernel)
+	pushq	%rdi
+
+#ifdef CONFIG_PARAVIRT
+	PV_SAVE_REGS(CLBR_RDI)
+#endif
+
+#ifdef CONFIG_PAX_KERNEXEC
+	GET_CR0_INTO_RDI
+	bts	$X86_CR0_WP_BIT,%rdi
+	jnc	3f
+	mov	%cs,%edi
+	cmp	$__KERNEL_CS,%edi
+	jnz	2f
+1:
+#endif
+
+#ifdef CONFIG_PAX_MEMORY_UDEREF
+	ALTERNATIVE "jmp 111f", "", X86_FEATURE_PCID
+	GET_CR3_INTO_RDI
+	cmp	$0,%dil
+	jnz	112f
+	mov	$__KERNEL_DS,%edi
+	mov	%edi,%ss
+	jmp	111f
+112:	cmp	$1,%dil
+	jz	113f
+	ud2
+113:	sub	$4097,%rdi
+	bts	$63,%rdi
+	SET_RDI_INTO_CR3
+	mov	$__UDEREF_KERNEL_DS,%edi
+	mov	%edi,%ss
+111:
+#endif
+
+#ifdef CONFIG_PARAVIRT
+	PV_RESTORE_REGS(CLBR_RDI)
+#endif
+
+	popq	%rdi
+	pax_ret pax_enter_kernel
+
+#ifdef CONFIG_PAX_KERNEXEC
+2:	ljmpq	__KERNEL_CS,1b
+3:	ljmpq	__KERNEXEC_KERNEL_CS,4f
+4:	SET_RDI_INTO_CR0
+	jmp	1b
+#endif
+ENDPROC(pax_enter_kernel)
+
+ENTRY(pax_exit_kernel)
+	pushq	%rdi
+
+#ifdef CONFIG_PARAVIRT
+	PV_SAVE_REGS(CLBR_RDI)
+#endif
+
+#ifdef CONFIG_PAX_KERNEXEC
+	mov	%cs,%rdi
+	cmp	$__KERNEXEC_KERNEL_CS,%edi
+	jz	2f
+	GET_CR0_INTO_RDI
+	bts	$X86_CR0_WP_BIT,%rdi
+	jnc	4f
+1:
+#endif
+
+#ifdef CONFIG_PAX_MEMORY_UDEREF
+	ALTERNATIVE "jmp 111f", "", X86_FEATURE_PCID
+	mov	%ss,%edi
+	cmp	$__UDEREF_KERNEL_DS,%edi
+	jnz	111f
+	GET_CR3_INTO_RDI
+	cmp	$0,%dil
+	jz	112f
+	ud2
+112:	add	$4097,%rdi
+	bts	$63,%rdi
+	SET_RDI_INTO_CR3
+	mov	$__KERNEL_DS,%edi
+	mov	%edi,%ss
+111:
+#endif
+
+#ifdef CONFIG_PARAVIRT
+	PV_RESTORE_REGS(CLBR_RDI);
+#endif
+
+	popq	%rdi
+	pax_ret pax_exit_kernel
+
+#ifdef CONFIG_PAX_KERNEXEC
+2:	GET_CR0_INTO_RDI
+	btr	$X86_CR0_WP_BIT,%rdi
+	jnc	4f
+	ljmpq	__KERNEL_CS,3f
+3:	SET_RDI_INTO_CR0
+	jmp	1b
+4:	ud2
+	jmp	4b
+#endif
+ENDPROC(pax_exit_kernel)
+#endif
+
+	.macro pax_enter_kernel_user
+	pax_set_fptr_mask
+#ifdef CONFIG_PAX_MEMORY_UDEREF
+	pax_direct_call pax_enter_kernel_user
+#endif
+	.endm
+
+	.macro pax_exit_kernel_user
+#ifdef CONFIG_PAX_MEMORY_UDEREF
+	pax_direct_call pax_exit_kernel_user
+#endif
+#ifdef CONFIG_PAX_RANDKSTACK
+	pushq	%rax
+	pushq	%r11
+	pax_direct_call pax_randomize_kstack
+	popq	%r11
+	popq	%rax
+#endif
+	.endm
+
+#ifdef CONFIG_PAX_MEMORY_UDEREF
+ENTRY(pax_enter_kernel_user)
+GLOBAL(patch_pax_enter_kernel_user)
+	pushq	%rdi
+	pushq	%rbx
+
+#ifdef CONFIG_PARAVIRT
+	PV_SAVE_REGS(CLBR_RDI)
+#endif
+
+	ALTERNATIVE "jmp 111f", "", X86_FEATURE_PCID
+	GET_CR3_INTO_RDI
+	cmp	$1,%dil
+	jnz	4f
+	sub	$4097,%rdi
+	bts	$63,%rdi
+	SET_RDI_INTO_CR3
+	jmp	3f
+111:
+
+	GET_CR3_INTO_RDI
+	mov	%rdi,%rbx
+	add	$__START_KERNEL_map,%rbx
+	sub	phys_base(%rip),%rbx
+
+#ifdef CONFIG_PARAVIRT
+	pushq %rdi
+	i = 0
+	.rept USER_PGD_PTRS
+	mov	i*8(%rbx),%rsi
+	mov	$0,%sil
+	lea	i*8(%rbx),%rdi
+	pax_indirect_call PARA_INDIRECT(pv_mmu_ops+PV_MMU_set_pgd_batched), pv_mmu_ops.set_pgd_batched
+	i = i + 1
+	.endr
+	popq	%rdi
+#else
+	i = 0
+	.rept USER_PGD_PTRS
+	movb	$0,i*8(%rbx)
+	i = i + 1
+	.endr
+#endif
+
+	SET_RDI_INTO_CR3
+
+#ifdef CONFIG_PAX_KERNEXEC
+	GET_CR0_INTO_RDI
+	bts	$X86_CR0_WP_BIT,%rdi
+	SET_RDI_INTO_CR0
+#endif
+
+3:
+
+#ifdef CONFIG_PARAVIRT
+	PV_RESTORE_REGS(CLBR_RDI)
+#endif
+
+	popq	%rbx
+	popq	%rdi
+	pax_ret pax_enter_kernel_user
+4:	ud2
+ENDPROC(pax_enter_kernel_user)
+
+ENTRY(pax_exit_kernel_user)
+GLOBAL(patch_pax_exit_kernel_user)
+	pushq	%rdi
+	pushq	%rbx
+
+#ifdef CONFIG_PARAVIRT
+	PV_SAVE_REGS(CLBR_RDI)
+#endif
+
+	GET_CR3_INTO_RDI
+	ALTERNATIVE "jmp 1f", "", X86_FEATURE_PCID
+	cmp	$0,%dil
+	jnz	3f
+	add	$4097,%rdi
+	bts	$63,%rdi
+	SET_RDI_INTO_CR3
+	jmp	2f
+1:
+
+	mov	%rdi,%rbx
+
+#ifdef CONFIG_PAX_KERNEXEC
+	GET_CR0_INTO_RDI
+	btr	$X86_CR0_WP_BIT,%rdi
+	jnc	3f
+	SET_RDI_INTO_CR0
+#endif
+
+	add	$__START_KERNEL_map,%rbx
+	sub	phys_base(%rip),%rbx
+
+#ifdef CONFIG_PARAVIRT
+	i = 0
+	.rept USER_PGD_PTRS
+	mov	i*8(%rbx),%rsi
+	mov	$0x67,%sil
+	lea	i*8(%rbx),%rdi
+	pax_indirect_call PARA_INDIRECT(pv_mmu_ops+PV_MMU_set_pgd_batched), pv_mmu_ops.set_pgd_batched
+	i = i + 1
+	.endr
+#else
+	i = 0
+	.rept USER_PGD_PTRS
+	movb	$0x67,i*8(%rbx)
+	i = i + 1
+	.endr
+#endif
+
+2:
+
+#ifdef CONFIG_PARAVIRT
+	PV_RESTORE_REGS(CLBR_RDI)
+#endif
+
+	popq	%rbx
+	popq	%rdi
+	pax_ret pax_exit_kernel_user
+3:	ud2
+ENDPROC(pax_exit_kernel_user)
+#endif
+
+	.macro pax_enter_kernel_nmi
+	pax_set_fptr_mask
+
+#ifdef CONFIG_PAX_KERNEXEC
+	GET_CR0_INTO_RDI
+	bts	$X86_CR0_WP_BIT,%rdi
+	jc	110f
+	SET_RDI_INTO_CR0
+	or	$2,%ebx
+110:
+#endif
+
+#ifdef CONFIG_PAX_MEMORY_UDEREF
+	ALTERNATIVE "jmp 111f", "", X86_FEATURE_PCID
+	GET_CR3_INTO_RDI
+	cmp	$0,%dil
+	jz	111f
+	sub	$4097,%rdi
+	or	$4,%ebx
+	bts	$63,%rdi
+	SET_RDI_INTO_CR3
+	mov	$__UDEREF_KERNEL_DS,%edi
+	mov	%edi,%ss
+111:
+#endif
+	.endm
+
+	.macro pax_exit_kernel_nmi
+#ifdef CONFIG_PAX_KERNEXEC
+	btr	$1,%ebx
+	jnc	110f
+	GET_CR0_INTO_RDI
+	btr	$X86_CR0_WP_BIT,%rdi
+	SET_RDI_INTO_CR0
+110:
+#endif
+
+#ifdef CONFIG_PAX_MEMORY_UDEREF
+	ALTERNATIVE "jmp 111f", "", X86_FEATURE_PCID
+	btr	$2,%ebx
+	jnc	111f
+	GET_CR3_INTO_RDI
+	add	$4097,%rdi
+	bts	$63,%rdi
+	SET_RDI_INTO_CR3
+	mov	$__KERNEL_DS,%edi
+	mov	%edi,%ss
+111:
+#endif
+	.endm
+
+	.macro pax_erase_kstack
+#ifdef CONFIG_PAX_MEMORY_STACKLEAK
+	pax_direct_call pax_erase_kstack
+#endif
+	.endm
+
+#ifdef CONFIG_PAX_MEMORY_STACKLEAK
+ENTRY(pax_erase_kstack)
+	pushq	%rdi
+	pushq	%rcx
+	pushq	%rax
+	pushq	%r11
+
+	GET_CURRENT(%r11)
+	mov	TASK_lowest_stack(%r11), %rdi
+	mov	$-0xBEEF, %rax
+	std
+
+1:	mov	%edi, %ecx
+	and	$THREAD_SIZE_asm - 1, %ecx
+	shr	$3, %ecx
+	repne	scasq
+	jecxz	2f
+
+	cmp	$2*8, %ecx
+	jc	2f
+
+	mov	$2*8, %ecx
+	repe	scasq
+	jecxz	2f
+	jne	1b
+
+2:	cld
+	or	$2*8, %rdi
+	mov	%esp, %ecx
+	sub	%edi, %ecx
+
+	cmp	$THREAD_SIZE_asm, %rcx
+	jb	3f
+	ud2
+3:
+
+	shr	$3, %ecx
+	rep	stosq
+
+	mov	TASK_thread_sp0(%r11), %rdi
+	sub	$256, %rdi
+	mov	%rdi, TASK_lowest_stack(%r11)
+
+	popq	%r11
+	popq	%rax
+	popq	%rcx
+	popq	%rdi
+	pax_ret pax_erase_kstack
+ENDPROC(pax_erase_kstack)
+#endif
+
 .macro TRACE_IRQS_IRETQ
 #ifdef CONFIG_TRACE_IRQFLAGS
 	bt	$9, EFLAGS(%rsp)		/* interrupts off? */
@@ -77,19 +466,19 @@ ENDPROC(native_usergs_sysret64)
 #if defined(CONFIG_DYNAMIC_FTRACE) && defined(CONFIG_TRACE_IRQFLAGS)
 
 .macro TRACE_IRQS_OFF_DEBUG
-	call	debug_stack_set_zero
+	pax_direct_call debug_stack_set_zero
 	TRACE_IRQS_OFF
-	call	debug_stack_reset
+	pax_direct_call debug_stack_reset
 .endm
 
 .macro TRACE_IRQS_ON_DEBUG
-	call	debug_stack_set_zero
+	pax_direct_call debug_stack_set_zero
 	TRACE_IRQS_ON
-	call	debug_stack_reset
+	pax_direct_call debug_stack_reset
 .endm
 
 .macro TRACE_IRQS_IRETQ_DEBUG
-	bt	$9, EFLAGS(%rsp)		/* interrupts off? */
+	bt	$X86_EFLAGS_IF_BIT, EFLAGS(%rsp)	/* interrupts off? */
 	jnc	1f
 	TRACE_IRQS_ON_DEBUG
 1:
@@ -176,6 +565,16 @@ GLOBAL(entry_SYSCALL_64_after_swapgs)
 	pushq	%r11				/* pt_regs->r11 */
 	sub	$(6*8), %rsp			/* pt_regs->bp, bx, r12-15 not saved */
 
+#ifdef CONFIG_PAX_KERNEXEC_PLUGIN_METHOD_OR
+	movq	%r12, R12(%rsp)
+#endif
+
+	pax_enter_kernel_user
+
+#ifdef CONFIG_PAX_RANDKSTACK
+	pax_erase_kstack
+#endif
+
 	/*
 	 * If we need to do entry work or if we guess we'll need to do
 	 * exit work, go straight to the slow path.
@@ -206,7 +605,7 @@ entry_SYSCALL_64_fastpath:
 	 * It might end up jumping to the slow path.  If it jumps, RAX
 	 * and all argument registers are clobbered.
 	 */
-	call	*sys_call_table(, %rax, 8)
+	pax_indirect_call "sys_call_table(, %rax, 8)", sys_ni_syscall
 .Lentry_SYSCALL_64_after_fastpath_call:
 
 	movq	%rax, RAX(%rsp)
@@ -223,6 +622,9 @@ entry_SYSCALL_64_fastpath:
 	testl	$_TIF_ALLWORK_MASK, TASK_TI_flags(%r11)
 	jnz	1f
 
+	pax_exit_kernel_user
+	pax_erase_kstack
+
 	LOCKDEP_SYS_EXIT
 	TRACE_IRQS_ON		/* user mode is traced as IRQs on */
 	movq	RIP(%rsp), %rcx
@@ -241,16 +643,19 @@ entry_SYSCALL_64_fastpath:
 	ENABLE_INTERRUPTS(CLBR_NONE)
 	SAVE_EXTRA_REGS
 	movq	%rsp, %rdi
-	call	syscall_return_slowpath	/* returns with IRQs disabled */
+	pax_direct_call syscall_return_slowpath	/* returns with IRQs disabled */
 	jmp	return_from_SYSCALL_64
 
 entry_SYSCALL64_slow_path:
 	/* IRQs are off. */
 	SAVE_EXTRA_REGS
 	movq	%rsp, %rdi
-	call	do_syscall_64		/* returns with IRQs disabled */
+	pax_direct_call do_syscall_64		/* returns with IRQs disabled */
 
 return_from_SYSCALL_64:
+	pax_exit_kernel_user
+	pax_erase_kstack
+
 	RESTORE_EXTRA_REGS
 	TRACE_IRQS_IRETQ		/* we're about to change IF */
 
@@ -275,13 +680,12 @@ return_from_SYSCALL_64:
 	.error "virtual address width changed -- SYSRET checks need update"
 	.endif
 
-	/* Change top 16 bits to be the sign-extension of 47th bit */
-	shl	$(64 - (__VIRTUAL_MASK_SHIFT+1)), %rcx
-	sar	$(64 - (__VIRTUAL_MASK_SHIFT+1)), %rcx
-
-	/* If this changed %rcx, it was not canonical */
-	cmpq	%rcx, %r11
-	jne	opportunistic_sysret_failed
+	/*
+	 * If the top 17 bits are not 0 then RIP isn't a userland address,
+	 * it may not even be canonical, fall back to iret
+	 */
+	shr	$(__VIRTUAL_MASK_SHIFT), %r11
+	jnz	opportunistic_sysret_failed
 
 	cmpq	$__USER_CS, CS(%rsp)		/* CS must match SYSRET */
 	jne	opportunistic_sysret_failed
@@ -329,7 +733,7 @@ syscall_return_via_sysret:
 opportunistic_sysret_failed:
 	SWAPGS
 	jmp	restore_c_regs_and_iret
-END(entry_SYSCALL_64)
+ENDPROC(entry_SYSCALL_64)
 
 ENTRY(stub_ptregs_64)
 	/*
@@ -355,13 +759,17 @@ ENTRY(stub_ptregs_64)
 
 1:
 	jmp	*%rax				/* Called from C */
-END(stub_ptregs_64)
+ENDPROC(stub_ptregs_64)
 
 .macro ptregs_stub func
-ENTRY(ptregs_\func)
+RAP_ENTRY(ptregs_\func)
+#ifdef CONFIG_PAX_RAP
+	leaq	rap_\func(%rip), %rax
+#else
 	leaq	\func(%rip), %rax
+#endif
 	jmp	stub_ptregs_64
-END(ptregs_\func)
+ENDPROC(ptregs_\func)
 .endm
 
 /* Instantiate ptregs_stub for each ptregs-using syscall */
@@ -381,7 +789,9 @@ ENTRY(__switch_to_asm)
 	 */
 	pushq	%rbp
 	pushq	%rbx
+#ifndef CONFIG_PAX_KERNEXEC_PLUGIN_METHOD_OR
 	pushq	%r12
+#endif
 	pushq	%r13
 	pushq	%r14
 	pushq	%r15
@@ -399,38 +809,49 @@ ENTRY(__switch_to_asm)
 	popq	%r15
 	popq	%r14
 	popq	%r13
+#ifndef CONFIG_PAX_KERNEXEC_PLUGIN_METHOD_OR
 	popq	%r12
+#endif
 	popq	%rbx
 	popq	%rbp
 
 	jmp	__switch_to
-END(__switch_to_asm)
+ENDPROC(__switch_to_asm)
 
 /*
  * A newly forked process directly context switches into this address.
  *
  * rax: prev task we switched from
  * rbx: kernel thread func (NULL for user thread)
- * r12: kernel thread arg
+ * r13: kernel thread arg
  */
+#ifdef CONFIG_PAX_RAP
+	__ALIGN
+	pax_retloc __switch_to
+	.globl ret_from_fork
+ret_from_fork:
+#else
 ENTRY(ret_from_fork)
+#endif
 	movq	%rax, %rdi
-	call	schedule_tail			/* rdi: 'prev' task parameter */
+	pax_direct_call schedule_tail		/* rdi: 'prev' task parameter */
 
 	testq	%rbx, %rbx			/* from kernel_thread? */
 	jnz	1f				/* kernel threads are uncommon */
 
 2:
 	movq	%rsp, %rdi
-	call	syscall_return_slowpath	/* returns with IRQs disabled */
+	pax_direct_call syscall_return_slowpath	/* returns with IRQs disabled */
+	pax_exit_kernel_user
+	pax_erase_kstack
 	TRACE_IRQS_ON			/* user mode is traced as IRQS on */
 	SWAPGS
 	jmp	restore_regs_and_iret
 
 1:
 	/* kernel thread */
-	movq	%r12, %rdi
-	call	*%rbx
+	movq	%r13, %rdi
+	pax_indirect_call %rbx, kthreadd
 	/*
 	 * A kernel thread is allowed to return here after successfully
 	 * calling do_execve().  Exit to userspace to complete the execve()
@@ -438,7 +859,7 @@ ENTRY(ret_from_fork)
 	 */
 	movq	$0, RAX(%rsp)
 	jmp	2b
-END(ret_from_fork)
+ENDPROC(ret_from_fork)
 
 /*
  * Build the entry stubs with some assembler magic.
@@ -453,7 +874,7 @@ ENTRY(irq_entries_start)
 	jmp	common_interrupt
 	.align	8
     .endr
-END(irq_entries_start)
+ENDPROC(irq_entries_start)
 
 /*
  * Interrupt entry/exit.
@@ -479,6 +900,12 @@ END(irq_entries_start)
 	 */
 	SWAPGS
 
+#ifdef CONFIG_PAX_MEMORY_UDEREF
+	pax_enter_kernel_user
+#else
+	pax_enter_kernel
+#endif
+
 	/*
 	 * We need to tell lockdep that IRQs are off.  We can't do this until
 	 * we fix gsbase, and we should do it before enter_from_user_mode
@@ -491,7 +918,9 @@ END(irq_entries_start)
 
 	CALL_enter_from_user_mode
 
-1:
+	jmp	2f
+1:	pax_enter_kernel
+2:
 	/*
 	 * Save previous stack pointer, optionally switch to interrupt stack.
 	 * irq_count is used to check if a CPU is already on an interrupt stack
@@ -503,10 +932,11 @@ END(irq_entries_start)
 	incl	PER_CPU_VAR(irq_count)
 	cmovzq	PER_CPU_VAR(irq_stack_ptr), %rsp
 	pushq	%rdi
+
 	/* We entered an interrupt context - irqs are off: */
 	TRACE_IRQS_OFF
 
-	call	\func	/* rdi points to pt_regs */
+	pax_direct_call \func	/* rdi points to pt_regs */
 	.endm
 
 	/*
@@ -533,7 +963,9 @@ ret_from_intr:
 	/* Interrupt came from user space */
 GLOBAL(retint_user)
 	mov	%rsp,%rdi
-	call	prepare_exit_to_usermode
+	pax_direct_call prepare_exit_to_usermode
+	pax_exit_kernel_user
+#	pax_erase_kstack
 	TRACE_IRQS_IRETQ
 	SWAPGS
 	jmp	restore_regs_and_iret
@@ -547,10 +979,25 @@ retint_kernel:
 	jnc	1f
 0:	cmpl	$0, PER_CPU_VAR(__preempt_count)
 	jnz	1f
-	call	preempt_schedule_irq
+	pax_direct_call preempt_schedule_irq
 	jmp	0b
 1:
 #endif
+
+	pax_exit_kernel
+
+#if defined(CONFIG_EFI) && defined(CONFIG_PAX_KERNEXEC_PLUGIN)
+	/* This is a quirk to allow IRQs/NMIs/MCEs during early EFI setup,
+	 * namely calling EFI runtime services with a phys mapping. We're
+	 * starting off with NOPs and patch in the real instrumentation
+	 * (BTS/OR) before starting any userland process; even before starting
+	 * up the APs.
+	 */
+	ALTERNATIVE "", "pax_force_retaddr 16*8", X86_FEATURE_ALWAYS
+#else
+	pax_force_retaddr RIP
+#endif
+
 	/*
 	 * The iretq could re-enable interrupts:
 	 */
@@ -614,15 +1061,15 @@ native_irq_return_ldt:
 	SWAPGS
 	movq	PER_CPU_VAR(espfix_waddr), %rdi
 	movq	%rax, (0*8)(%rdi)		/* user RAX */
-	movq	(1*8)(%rsp), %rax		/* user RIP */
+	movq	(8 + RIP-RIP)(%rsp), %rax	/* user RIP */
 	movq	%rax, (1*8)(%rdi)
-	movq	(2*8)(%rsp), %rax		/* user CS */
+	movq	(8 + CS-RIP)(%rsp), %rax	/* user CS */
 	movq	%rax, (2*8)(%rdi)
-	movq	(3*8)(%rsp), %rax		/* user RFLAGS */
+	movq	(8 + EFLAGS-RIP)(%rsp), %rax	/* user RFLAGS */
 	movq	%rax, (3*8)(%rdi)
-	movq	(5*8)(%rsp), %rax		/* user SS */
+	movq	(8 + SS-RIP)(%rsp), %rax	/* user SS */
 	movq	%rax, (5*8)(%rdi)
-	movq	(4*8)(%rsp), %rax		/* user RSP */
+	movq	(8 + RSP-RIP)(%rsp), %rax	/* user RSP */
 	movq	%rax, (4*8)(%rdi)
 	/* Now RAX == RSP. */
 
@@ -654,7 +1101,7 @@ native_irq_return_ldt:
 	 */
 	jmp	native_irq_return_iret
 #endif
-END(common_interrupt)
+ENDPROC(common_interrupt)
 
 /*
  * APIC interrupts.
@@ -666,7 +1113,7 @@ ENTRY(\sym)
 .Lcommon_\sym:
 	interrupt \do_sym
 	jmp	ret_from_intr
-END(\sym)
+ENDPROC(\sym)
 .endm
 
 #ifdef CONFIG_TRACING
@@ -742,15 +1189,19 @@ apicinterrupt IRQ_WORK_VECTOR			irq_work_interrupt		smp_irq_work_interrupt
 /*
  * Exception entry points.
  */
-#define CPU_TSS_IST(x) PER_CPU_VAR(cpu_tss) + (TSS_ist + ((x) - 1) * 8)
+#define CPU_TSS_IST(x) (TSS_ist + ((x) - 1) * 8)(%r13)
 
-.macro idtentry sym do_sym has_error_code:req paranoid=0 shift_ist=-1
+.macro idtentry sym do_sym has_error_code:req paranoid=0 shift_ist=-1 rap_hash=0
 ENTRY(\sym)
 	/* Sanity check */
 	.if \shift_ist != -1 && \paranoid == 0
 	.error "using shift_ist requires paranoid=1"
 	.endif
 
+	.if \paranoid != 0 && \rap_hash==tailcall
+	.error "tail called idt entry cannot be paranoid"
+	.endif
+
 	ASM_CLAC
 	PARAVIRT_ADJUST_EXCEPTION_FRAME
 
@@ -765,9 +1216,9 @@ ENTRY(\sym)
 	testb	$3, CS(%rsp)			/* If coming from userspace, switch stacks */
 	jnz	1f
 	.endif
-	call	paranoid_entry
+	pax_direct_call paranoid_entry
 	.else
-	call	error_entry
+	pax_direct_call error_entry
 	.endif
 	/* returned flag: ebx=0: need swapgs on exit, ebx=1: don't need it */
 
@@ -789,10 +1240,23 @@ ENTRY(\sym)
 	.endif
 
 	.if \shift_ist != -1
+#ifdef CONFIG_SMP
+	imul	$TSS_size, PER_CPU_VAR(cpu_number), %r13d
+	leaq	cpu_tss(%r13), %r13
+#else
+	leaq	cpu_tss(%rip), %r13
+#endif
 	subq	$EXCEPTION_STKSZ, CPU_TSS_IST(\shift_ist)
 	.endif
 
-	call	\do_sym
+	.ifc \rap_hash,tailcall
+	jmp \do_sym
+	.exitm
+	.elseif \rap_hash == 0
+	pax_direct_call \do_sym
+	.else
+	pax_indirect_call \do_sym, \rap_hash
+	.endif
 
 	.if \shift_ist != -1
 	addq	$EXCEPTION_STKSZ, CPU_TSS_IST(\shift_ist)
@@ -812,11 +1276,11 @@ ENTRY(\sym)
 	 * run in real process context if user_mode(regs).
 	 */
 1:
-	call	error_entry
+	pax_direct_call error_entry
 
 
 	movq	%rsp, %rdi			/* pt_regs pointer */
-	call	sync_regs
+	pax_direct_call sync_regs
 	movq	%rax, %rsp			/* switch stack */
 
 	movq	%rsp, %rdi			/* pt_regs pointer */
@@ -828,11 +1292,15 @@ ENTRY(\sym)
 	xorl	%esi, %esi			/* no error code */
 	.endif
 
-	call	\do_sym
+	.if \rap_hash == 0
+	pax_direct_call \do_sym
+	.else
+	pax_indirect_call \do_sym, \rap_hash
+	.endif
 
 	jmp	error_exit			/* %ebx: no swapgs flag */
 	.endif
-END(\sym)
+ENDPROC(\sym)
 .endm
 
 #ifdef CONFIG_TRACING
@@ -860,6 +1328,14 @@ idtentry coprocessor_error		do_coprocessor_error		has_error_code=0
 idtentry alignment_check		do_alignment_check		has_error_code=1
 idtentry simd_coprocessor_error		do_simd_coprocessor_error	has_error_code=0
 
+#ifdef CONFIG_PAX_REFCOUNT
+idtentry refcount_error			do_refcount_error		has_error_code=0
+#endif
+
+#ifdef CONFIG_PAX_RAP
+idtentry rap_call_error			do_rap_call_error		has_error_code=0
+idtentry rap_ret_error			do_rap_ret_error		has_error_code=0
+#endif
 
 	/*
 	 * Reload gs selector with exception handling
@@ -874,8 +1350,8 @@ ENTRY(native_load_gs_index)
 2:	ALTERNATIVE "", "mfence", X86_BUG_SWAPGS_FENCE
 	SWAPGS
 	popfq
-	ret
-END(native_load_gs_index)
+	pax_ret native_load_gs_index
+ENDPROC(native_load_gs_index)
 EXPORT_SYMBOL(native_load_gs_index)
 
 	_ASM_EXTABLE(.Lgs_change, bad_gs)
@@ -901,14 +1377,14 @@ ENTRY(do_softirq_own_stack)
 	incl	PER_CPU_VAR(irq_count)
 	cmove	PER_CPU_VAR(irq_stack_ptr), %rsp
 	push	%rbp				/* frame pointer backlink */
-	call	__do_softirq
+	pax_direct_call __do_softirq
 	leaveq
 	decl	PER_CPU_VAR(irq_count)
-	ret
-END(do_softirq_own_stack)
+	pax_ret do_softirq_own_stack
+ENDPROC(do_softirq_own_stack)
 
 #ifdef CONFIG_XEN
-idtentry xen_hypervisor_callback xen_do_hypervisor_callback has_error_code=0
+idtentry xen_hypervisor_callback xen_do_hypervisor_callback has_error_code=0 rap_hash=tailcall
 
 /*
  * A note on the "critical region" in our callback handler.
@@ -929,19 +1405,18 @@ ENTRY(xen_do_hypervisor_callback)		/* do_hypervisor_callback(struct *pt_regs) */
  * Since we don't modify %rdi, evtchn_do_upall(struct *pt_regs) will
  * see the correct pointer to the pt_regs
  */
-	movq	%rdi, %rsp			/* we don't return, adjust the stack frame */
 11:	incl	PER_CPU_VAR(irq_count)
 	movq	%rsp, %rbp
 	cmovzq	PER_CPU_VAR(irq_stack_ptr), %rsp
 	pushq	%rbp				/* frame pointer backlink */
-	call	xen_evtchn_do_upcall
+	pax_direct_call xen_evtchn_do_upcall
 	popq	%rsp
 	decl	PER_CPU_VAR(irq_count)
 #ifndef CONFIG_PREEMPT
-	call	xen_maybe_preempt_hcall
+	pax_direct_call xen_maybe_preempt_hcall
 #endif
 	jmp	error_exit
-END(xen_do_hypervisor_callback)
+ENDPROC(xen_do_hypervisor_callback)
 
 /*
  * Hypervisor uses this for application faults while it executes.
@@ -986,7 +1461,7 @@ ENTRY(xen_failsafe_callback)
 	SAVE_C_REGS
 	SAVE_EXTRA_REGS
 	jmp	error_exit
-END(xen_failsafe_callback)
+ENDPROC(xen_failsafe_callback)
 
 apicinterrupt3 HYPERVISOR_CALLBACK_VECTOR \
 	xen_hvm_callback_vector xen_evtchn_do_upcall
@@ -998,7 +1473,7 @@ apicinterrupt3 HYPERVISOR_CALLBACK_VECTOR \
 	hyperv_callback_vector hyperv_vector_handler
 #endif /* CONFIG_HYPERV */
 
-idtentry debug			do_debug		has_error_code=0	paranoid=1 shift_ist=DEBUG_STACK
+idtentry int1			do_debug		has_error_code=0	paranoid=1 shift_ist=DEBUG_STACK
 idtentry int3			do_int3			has_error_code=0	paranoid=1 shift_ist=DEBUG_STACK
 idtentry stack_segment		do_stack_segment	has_error_code=1
 
@@ -1016,7 +1491,7 @@ idtentry async_page_fault	do_async_page_fault	has_error_code=1
 #endif
 
 #ifdef CONFIG_X86_MCE
-idtentry machine_check					has_error_code=0	paranoid=1 do_sym=*machine_check_vector(%rip)
+idtentry machine_check					has_error_code=0	paranoid=1 do_sym="machine_check_vector(%rip)" rap_hash=do_machine_check
 #endif
 
 /*
@@ -1035,8 +1510,32 @@ ENTRY(paranoid_entry)
 	js	1f				/* negative -> in kernel */
 	SWAPGS
 	xorl	%ebx, %ebx
-1:	ret
-END(paranoid_entry)
+1:
+#ifdef CONFIG_PAX_MEMORY_UDEREF
+	testb	$3, CS+8(%rsp)
+	jz	1f
+	pax_enter_kernel_user
+	jmp	2f
+#endif
+1:	pax_enter_kernel
+2:
+	pax_ret paranoid_entry
+ENDPROC(paranoid_entry)
+
+ENTRY(paranoid_entry_nmi)
+	cld
+	SAVE_C_REGS 8
+	SAVE_EXTRA_REGS 8
+	movl	$1, %ebx
+	movl	$MSR_GS_BASE, %ecx
+	rdmsr
+	testl	%edx, %edx
+	js	1f	/* negative -> in kernel */
+	SWAPGS
+	xorl	%ebx, %ebx
+1:	pax_enter_kernel_nmi
+	pax_ret paranoid_entry_nmi
+ENDPROC(paranoid_entry_nmi)
 
 /*
  * "Paranoid" exit path from exception stack.  This is invoked
@@ -1053,19 +1552,26 @@ END(paranoid_entry)
 ENTRY(paranoid_exit)
 	DISABLE_INTERRUPTS(CLBR_NONE)
 	TRACE_IRQS_OFF_DEBUG
-	testl	%ebx, %ebx			/* swapgs needed? */
+	testl	$1, %ebx			/* swapgs needed? */
 	jnz	paranoid_exit_no_swapgs
+#ifdef CONFIG_PAX_MEMORY_UDEREF
+	pax_exit_kernel_user
+#else
+	pax_exit_kernel
+#endif
 	TRACE_IRQS_IRETQ
 	SWAPGS_UNSAFE_STACK
 	jmp	paranoid_exit_restore
 paranoid_exit_no_swapgs:
+	pax_exit_kernel
 	TRACE_IRQS_IRETQ_DEBUG
 paranoid_exit_restore:
 	RESTORE_EXTRA_REGS
 	RESTORE_C_REGS
 	REMOVE_PT_GPREGS_FROM_STACK 8
+	pax_force_retaddr_bts
 	INTERRUPT_RETURN
-END(paranoid_exit)
+ENDPROC(paranoid_exit)
 
 /*
  * Save all registers in pt_regs, and switch gs if needed.
@@ -1085,6 +1591,12 @@ ENTRY(error_entry)
 	 */
 	SWAPGS
 
+#ifdef CONFIG_PAX_MEMORY_UDEREF
+	pax_enter_kernel_user
+#else
+	pax_enter_kernel
+#endif
+
 .Lerror_entry_from_usermode_after_swapgs:
 	/*
 	 * We need to tell lockdep that IRQs are off.  We can't do this until
@@ -1093,11 +1605,11 @@ ENTRY(error_entry)
 	 */
 	TRACE_IRQS_OFF
 	CALL_enter_from_user_mode
-	ret
+	pax_ret error_entry
 
 .Lerror_entry_done:
 	TRACE_IRQS_OFF
-	ret
+	pax_ret error_entry
 
 	/*
 	 * There are two places in the kernel that can potentially fault with
@@ -1114,7 +1626,7 @@ ENTRY(error_entry)
 	cmpq	%rax, RIP+8(%rsp)
 	je	.Lbstep_iret
 	cmpq	$.Lgs_change, RIP+8(%rsp)
-	jne	.Lerror_entry_done
+	jne	1f
 
 	/*
 	 * hack: .Lgs_change can fail with user gsbase.  If this happens, fix up
@@ -1122,7 +1634,8 @@ ENTRY(error_entry)
 	 * .Lgs_change's error handler with kernel gsbase.
 	 */
 	SWAPGS
-	jmp .Lerror_entry_done
+1:	pax_enter_kernel
+	jmp	.Lerror_entry_done
 
 .Lbstep_iret:
 	/* Fix truncated RIP */
@@ -1136,17 +1649,23 @@ ENTRY(error_entry)
 	 */
 	SWAPGS
 
+#ifdef CONFIG_PAX_MEMORY_UDEREF
+	pax_enter_kernel_user
+#else
+	pax_enter_kernel
+#endif
+
 	/*
 	 * Pretend that the exception came from user mode: set up pt_regs
 	 * as if we faulted immediately after IRET and clear EBX so that
 	 * error_exit knows that we will be returning to user mode.
 	 */
 	mov	%rsp, %rdi
-	call	fixup_bad_iret
+	pax_direct_call fixup_bad_iret
 	mov	%rax, %rsp
 	decl	%ebx
 	jmp	.Lerror_entry_from_usermode_after_swapgs
-END(error_entry)
+ENDPROC(error_entry)
 
 
 /*
@@ -1158,10 +1677,10 @@ ENTRY(error_exit)
 	movl	%ebx, %eax
 	DISABLE_INTERRUPTS(CLBR_NONE)
 	TRACE_IRQS_OFF
-	testl	%eax, %eax
+	testl	$1, %eax
 	jnz	retint_kernel
 	jmp	retint_user
-END(error_exit)
+ENDPROC(error_exit)
 
 /* Runs on exception stack */
 ENTRY(nmi)
@@ -1215,6 +1734,8 @@ ENTRY(nmi)
 	 * other IST entries.
 	 */
 
+	ASM_CLAC
+
 	/* Use %rdx as our temp variable throughout */
 	pushq	%rdx
 
@@ -1258,6 +1779,12 @@ ENTRY(nmi)
 	pushq	%r14		/* pt_regs->r14 */
 	pushq	%r15		/* pt_regs->r15 */
 
+#if defined(CONFIG_PAX_KERNEXEC) || defined(CONFIG_PAX_MEMORY_UDEREF)
+	xorl	%ebx, %ebx
+#endif
+
+	pax_enter_kernel_nmi
+
 	/*
 	 * At this point we no longer need to worry about stack damage
 	 * due to nesting -- we're on the normal thread stack and we're
@@ -1266,7 +1793,9 @@ ENTRY(nmi)
 
 	movq	%rsp, %rdi
 	movq	$-1, %rsi
-	call	do_nmi
+	pax_direct_call do_nmi
+
+	pax_exit_kernel_nmi
 
 	/*
 	 * Return back to user mode.  We must *not* do the normal exit
@@ -1274,6 +1803,11 @@ ENTRY(nmi)
 	 * do_nmi doesn't modify pt_regs.
 	 */
 	SWAPGS
+
+#if defined(CONFIG_PAX_KERNEXEC) || defined(CONFIG_PAX_MEMORY_UDEREF)
+	movq	RBX(%rsp), %rbx
+#endif
+
 	jmp	restore_c_regs_and_iret
 
 .Lnmi_from_kernel:
@@ -1395,6 +1929,7 @@ nested_nmi_out:
 	popq	%rdx
 
 	/* We are returning to kernel mode, so this cannot result in a fault. */
+#	pax_force_retaddr_bts
 	INTERRUPT_RETURN
 
 first_nmi:
@@ -1423,7 +1958,7 @@ first_nmi:
 	pushq	%rsp		/* RSP (minus 8 because of the previous push) */
 	addq	$8, (%rsp)	/* Fix up RSP */
 	pushfq			/* RFLAGS */
-	pushq	$__KERNEL_CS	/* CS */
+	pushq	4*8(%rsp)	/* CS */
 	pushq	$1f		/* RIP */
 	INTERRUPT_RETURN	/* continues at repeat_nmi below */
 1:
@@ -1468,20 +2003,22 @@ end_repeat_nmi:
 	ALLOC_PT_GPREGS_ON_STACK
 
 	/*
-	 * Use paranoid_entry to handle SWAPGS, but no need to use paranoid_exit
+	 * Use paranoid_entry_nmi to handle SWAPGS, but no need to use paranoid_exit
 	 * as we should not be calling schedule in NMI context.
 	 * Even with normal interrupts enabled. An NMI should not be
 	 * setting NEED_RESCHED or anything that normal interrupts and
 	 * exceptions might do.
 	 */
-	call	paranoid_entry
+	pax_direct_call paranoid_entry_nmi
 
 	/* paranoidentry do_nmi, 0; without TRACE_IRQS_OFF */
 	movq	%rsp, %rdi
 	movq	$-1, %rsi
-	call	do_nmi
+	pax_direct_call do_nmi
 
-	testl	%ebx, %ebx			/* swapgs needed? */
+	pax_exit_kernel_nmi
+
+	testl	$1, %ebx			/* swapgs needed? */
 	jnz	nmi_restore
 nmi_swapgs:
 	SWAPGS_UNSAFE_STACK
@@ -1492,6 +2029,8 @@ nmi_restore:
 	/* Point RSP at the "iret" frame. */
 	REMOVE_PT_GPREGS_FROM_STACK 6*8
 
+	pax_force_retaddr_bts
+
 	/*
 	 * Clear "NMI executing".  Set DF first so that we can easily
 	 * distinguish the remaining code between here and IRET from
@@ -1509,12 +2048,12 @@ nmi_restore:
 	 * mode, so this cannot result in a fault.
 	 */
 	INTERRUPT_RETURN
-END(nmi)
+ENDPROC(nmi)
 
 ENTRY(ignore_sysret)
 	mov	$-ENOSYS, %eax
 	sysret
-END(ignore_sysret)
+ENDPROC(ignore_sysret)
 
 ENTRY(rewind_stack_do_exit)
 	/* Prevent any naive code from trying to unwind to our caller. */
@@ -1523,6 +2062,6 @@ ENTRY(rewind_stack_do_exit)
 	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rax
 	leaq	-TOP_OF_KERNEL_STACK_PADDING-PTREGS_SIZE(%rax), %rsp
 
-	call	do_exit
+	pax_direct_call do_group_exit
 1:	jmp 1b
-END(rewind_stack_do_exit)
+ENDPROC(rewind_stack_do_exit)

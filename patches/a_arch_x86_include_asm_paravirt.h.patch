diff --git a/arch/x86/include/asm/paravirt.h b/arch/x86/include/asm/paravirt.h
index ce93281..17b0fda 100644
--- a/arch/x86/include/asm/paravirt.h
+++ b/arch/x86/include/asm/paravirt.h
@@ -374,11 +374,11 @@ static inline pte_t __pte(pteval_t val)
 
 	if (sizeof(pteval_t) > sizeof(long))
 		ret = PVOP_CALLEE2(pteval_t,
-				   pv_mmu_ops.make_pte,
+				   pv_mmu_ops, make_pte,
 				   val, (u64)val >> 32);
 	else
 		ret = PVOP_CALLEE1(pteval_t,
-				   pv_mmu_ops.make_pte,
+				   pv_mmu_ops, make_pte,
 				   val);
 
 	return (pte_t) { .pte = ret };
@@ -389,10 +389,10 @@ static inline pteval_t pte_val(pte_t pte)
 	pteval_t ret;
 
 	if (sizeof(pteval_t) > sizeof(long))
-		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops.pte_val,
+		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops, pte_val,
 				   pte.pte, (u64)pte.pte >> 32);
 	else
-		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops.pte_val,
+		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops, pte_val,
 				   pte.pte);
 
 	return ret;
@@ -403,10 +403,10 @@ static inline pgd_t __pgd(pgdval_t val)
 	pgdval_t ret;
 
 	if (sizeof(pgdval_t) > sizeof(long))
-		ret = PVOP_CALLEE2(pgdval_t, pv_mmu_ops.make_pgd,
+		ret = PVOP_CALLEE2(pgdval_t, pv_mmu_ops, make_pgd,
 				   val, (u64)val >> 32);
 	else
-		ret = PVOP_CALLEE1(pgdval_t, pv_mmu_ops.make_pgd,
+		ret = PVOP_CALLEE1(pgdval_t, pv_mmu_ops, make_pgd,
 				   val);
 
 	return (pgd_t) { ret };
@@ -417,10 +417,10 @@ static inline pgdval_t pgd_val(pgd_t pgd)
 	pgdval_t ret;
 
 	if (sizeof(pgdval_t) > sizeof(long))
-		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
+		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops, pgd_val,
 				    pgd.pgd, (u64)pgd.pgd >> 32);
 	else
-		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
+		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops, pgd_val,
 				    pgd.pgd);
 
 	return ret;
@@ -496,24 +496,24 @@ static inline pmd_t __pmd(pmdval_t val)
 	pmdval_t ret;
 
 	if (sizeof(pmdval_t) > sizeof(long))
-		ret = PVOP_CALLEE2(pmdval_t, pv_mmu_ops.make_pmd,
+		ret = PVOP_CALLEE2(pmdval_t, pv_mmu_ops, make_pmd,
 				   val, (u64)val >> 32);
 	else
-		ret = PVOP_CALLEE1(pmdval_t, pv_mmu_ops.make_pmd,
+		ret = PVOP_CALLEE1(pmdval_t, pv_mmu_ops, make_pmd,
 				   val);
 
 	return (pmd_t) { ret };
 }
 
-static inline pmdval_t pmd_val(pmd_t pmd)
+static inline __intentional_overflow(-1) pmdval_t pmd_val(pmd_t pmd)
 {
 	pmdval_t ret;
 
 	if (sizeof(pmdval_t) > sizeof(long))
-		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
+		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops, pmd_val,
 				    pmd.pmd, (u64)pmd.pmd >> 32);
 	else
-		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
+		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops, pmd_val,
 				    pmd.pmd);
 
 	return ret;
@@ -536,10 +536,10 @@ static inline pud_t __pud(pudval_t val)
 	pudval_t ret;
 
 	if (sizeof(pudval_t) > sizeof(long))
-		ret = PVOP_CALLEE2(pudval_t, pv_mmu_ops.make_pud,
+		ret = PVOP_CALLEE2(pudval_t, pv_mmu_ops, make_pud,
 				   val, (u64)val >> 32);
 	else
-		ret = PVOP_CALLEE1(pudval_t, pv_mmu_ops.make_pud,
+		ret = PVOP_CALLEE1(pudval_t, pv_mmu_ops, make_pud,
 				   val);
 
 	return (pud_t) { ret };
@@ -550,10 +550,10 @@ static inline pudval_t pud_val(pud_t pud)
 	pudval_t ret;
 
 	if (sizeof(pudval_t) > sizeof(long))
-		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
+		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops, pud_val,
 				    pud.pud, (u64)pud.pud >> 32);
 	else
-		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
+		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops, pud_val,
 				    pud.pud);
 
 	return ret;
@@ -571,6 +571,18 @@ static inline void set_pgd(pgd_t *pgdp, pgd_t pgd)
 			    val);
 }
 
+static inline void set_pgd_batched(pgd_t *pgdp, pgd_t pgd)
+{
+	pgdval_t val = native_pgd_val(pgd);
+
+	if (sizeof(pgdval_t) > sizeof(long))
+		PVOP_VCALL3(pv_mmu_ops.set_pgd_batched, pgdp,
+			    val, (u64)val >> 32);
+	else
+		PVOP_VCALL2(pv_mmu_ops.set_pgd_batched, pgdp,
+			    val);
+}
+
 static inline void pgd_clear(pgd_t *pgdp)
 {
 	set_pgd(pgdp, __pgd(0));
@@ -655,6 +667,21 @@ static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
 	pv_mmu_ops.set_fixmap(idx, phys, flags);
 }
 
+#ifdef CONFIG_PAX_KERNEXEC
+static inline unsigned long pax_open_kernel(void)
+{
+	return PVOP_CALL0(unsigned long, pv_mmu_ops.pax_open_kernel);
+}
+
+static inline unsigned long pax_close_kernel(void)
+{
+	return PVOP_CALL0(unsigned long, pv_mmu_ops.pax_close_kernel);
+}
+#else
+static inline unsigned long pax_open_kernel(void) { return 0; }
+static inline unsigned long pax_close_kernel(void) { return 0; }
+#endif
+
 #if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
 
 static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
@@ -665,7 +692,7 @@ static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
 
 static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)
 {
-	PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);
+	PVOP_VCALLEE1(pv_lock_ops, queued_spin_unlock, lock);
 }
 
 static __always_inline void pv_wait(u8 *ptr, u8 val)
@@ -735,7 +762,7 @@ static __always_inline void pv_kick(int cpu)
  */
 #define PV_THUNK_NAME(func) "__raw_callee_save_" #func
 #define PV_CALLEE_SAVE_REGS_THUNK(func)					\
-	extern typeof(func) __raw_callee_save_##func;			\
+	extern typeof(func) __raw_callee_save_##func __rap_hash;	\
 									\
 	asm(".pushsection .text;"					\
 	    ".globl " PV_THUNK_NAME(func) ";"				\
@@ -743,38 +770,42 @@ static __always_inline void pv_kick(int cpu)
 	    PV_THUNK_NAME(func) ":"					\
 	    FRAME_BEGIN							\
 	    PV_SAVE_ALL_CALLER_REGS					\
-	    "call " #func ";"						\
+	    PAX_DIRECT_CALL_HASH(#func, PV_THUNK_NAME(func)) ";"	\
 	    PV_RESTORE_ALL_CALLER_REGS					\
 	    FRAME_END							\
-	    "ret;"							\
+	    PAX_RET(PV_THUNK_NAME(func))";"				\
 	    ".popsection")
 
 /* Get a reference to a callee-save function */
-#define PV_CALLEE_SAVE(func)						\
-	((struct paravirt_callee_save) { __raw_callee_save_##func })
+#define PV_CALLEE_SAVE(field, func)					\
+	((union paravirt_callee_save) { .field = __raw_callee_save_##func })
 
+#ifdef CONFIG_PAX_RAP
+#define __PV_IS_CALLEE_SAVE(field, func) PV_CALLEE_SAVE(field, func)
+#else
 /* Promise that "func" already uses the right calling convention */
-#define __PV_IS_CALLEE_SAVE(func)			\
-	((struct paravirt_callee_save) { func })
+#define __PV_IS_CALLEE_SAVE(field, func)			\
+	((union paravirt_callee_save) { .field = func })
+#endif
 
 static inline notrace unsigned long arch_local_save_flags(void)
 {
-	return PVOP_CALLEE0(unsigned long, pv_irq_ops.save_fl);
+	return PVOP_CALLEE0(unsigned long, pv_irq_ops, save_fl);
 }
 
 static inline notrace void arch_local_irq_restore(unsigned long f)
 {
-	PVOP_VCALLEE1(pv_irq_ops.restore_fl, f);
+	PVOP_VCALLEE1(pv_irq_ops, restore_fl, f);
 }
 
 static inline notrace void arch_local_irq_disable(void)
 {
-	PVOP_VCALLEE0(pv_irq_ops.irq_disable);
+	PVOP_VCALLEE0(pv_irq_ops, irq_disable);
 }
 
 static inline notrace void arch_local_irq_enable(void)
 {
-	PVOP_VCALLEE0(pv_irq_ops.irq_enable);
+	PVOP_VCALLEE0(pv_irq_ops, irq_enable);
 }
 
 static inline notrace unsigned long arch_local_irq_save(void)
@@ -806,9 +837,9 @@ extern void default_banner(void);
 
 #else  /* __ASSEMBLY__ */
 
-#define _PVSITE(ptype, clobbers, ops, word, algn)	\
+#define _PVSITE(ptype, clobbers, word, algn, ...)\
 771:;						\
-	ops;					\
+	__VA_ARGS__;				\
 772:;						\
 	.pushsection .parainstructions,"a";	\
 	 .align	algn;				\
@@ -848,8 +879,10 @@ extern void default_banner(void);
 	COND_POP(set, CLBR_RAX, rax)
 
 #define PARA_PATCH(struct, off)        ((PARAVIRT_PATCH_##struct + (off)) / 8)
-#define PARA_SITE(ptype, clobbers, ops) _PVSITE(ptype, clobbers, ops, .quad, 8)
-#define PARA_INDIRECT(addr)	*addr(%rip)
+#define PARA_SITE(ptype, clobbers, ...) _PVSITE(ptype, clobbers, .quad, 8, __VA_ARGS__)
+#define PARA_INDIRECT(addr)	addr(%rip)
+#define PV_INDIRECT_CALL(ops, OPS, addr) pax_indirect_call PARA_INDIRECT(pv_##ops##_ops+PV_##OPS##_##addr), pv_##ops##_ops.##addr
+#define PV_INDIRECT_CALL_CALLEE_SAVE(ops, OPS, addr) pax_indirect_call PARA_INDIRECT(pv_##ops##_ops+PV_##OPS##_##addr), pv_##ops##_ops.##addr##.##addr
 #else
 #define PV_SAVE_REGS(set)			\
 	COND_PUSH(set, CLBR_EAX, eax);		\
@@ -863,30 +896,32 @@ extern void default_banner(void);
 	COND_POP(set, CLBR_EAX, eax)
 
 #define PARA_PATCH(struct, off)        ((PARAVIRT_PATCH_##struct + (off)) / 4)
-#define PARA_SITE(ptype, clobbers, ops) _PVSITE(ptype, clobbers, ops, .long, 4)
-#define PARA_INDIRECT(addr)	*%cs:addr
+#define PARA_SITE(ptype, clobbers, ...) _PVSITE(ptype, clobbers, .long, 4, __VA_ARGS__)
+#define PARA_INDIRECT(addr)	%ss:addr
+#define PV_INDIRECT_CALL(ops, OPS, addr) pax_indirect_call PARA_INDIRECT(pv_##ops##_ops+PV_##OPS##_##addr), pv_##ops##_ops.##addr
+#define PV_INDIRECT_CALL_CALLEE_SAVE(ops, OPS, addr) pax_indirect_call PARA_INDIRECT(pv_##ops##_ops+PV_##OPS##_##addr), pv_##ops##_ops.##addr##.##addr
 #endif
 
 #define INTERRUPT_RETURN						\
 	PARA_SITE(PARA_PATCH(pv_cpu_ops, PV_CPU_iret), CLBR_NONE,	\
-		  jmp PARA_INDIRECT(pv_cpu_ops+PV_CPU_iret))
+		  jmp *PARA_INDIRECT(pv_cpu_ops+PV_CPU_iret))
 
 #define DISABLE_INTERRUPTS(clobbers)					\
 	PARA_SITE(PARA_PATCH(pv_irq_ops, PV_IRQ_irq_disable), clobbers, \
 		  PV_SAVE_REGS(clobbers | CLBR_CALLEE_SAVE);		\
-		  call PARA_INDIRECT(pv_irq_ops+PV_IRQ_irq_disable);	\
+		  PV_INDIRECT_CALL_CALLEE_SAVE(irq,IRQ,irq_disable);	\
 		  PV_RESTORE_REGS(clobbers | CLBR_CALLEE_SAVE);)
 
 #define ENABLE_INTERRUPTS(clobbers)					\
 	PARA_SITE(PARA_PATCH(pv_irq_ops, PV_IRQ_irq_enable), clobbers,	\
 		  PV_SAVE_REGS(clobbers | CLBR_CALLEE_SAVE);		\
-		  call PARA_INDIRECT(pv_irq_ops+PV_IRQ_irq_enable);	\
+		  PV_INDIRECT_CALL_CALLEE_SAVE(irq,IRQ,irq_enable);	\
 		  PV_RESTORE_REGS(clobbers | CLBR_CALLEE_SAVE);)
 
 #ifdef CONFIG_X86_32
 #define GET_CR0_INTO_EAX				\
 	push %ecx; push %edx;				\
-	call PARA_INDIRECT(pv_cpu_ops+PV_CPU_read_cr0);	\
+	PV_INDIRECT_CALL(cpu,CPU,read_cr0);		\
 	pop %edx; pop %ecx
 #else	/* !CONFIG_X86_32 */
 
@@ -907,21 +942,36 @@ extern void default_banner(void);
  */
 #define SWAPGS								\
 	PARA_SITE(PARA_PATCH(pv_cpu_ops, PV_CPU_swapgs), CLBR_NONE,	\
-		  call PARA_INDIRECT(pv_cpu_ops+PV_CPU_swapgs)		\
+		  PV_INDIRECT_CALL(cpu,CPU,swapgs)			\
 		 )
 
 #define GET_CR2_INTO_RAX				\
-	call PARA_INDIRECT(pv_mmu_ops+PV_MMU_read_cr2)
+	PV_INDIRECT_CALL(mmu,MMU,read_cr2)
 
 #define PARAVIRT_ADJUST_EXCEPTION_FRAME					\
 	PARA_SITE(PARA_PATCH(pv_irq_ops, PV_IRQ_adjust_exception_frame), \
 		  CLBR_NONE,						\
-		  call PARA_INDIRECT(pv_irq_ops+PV_IRQ_adjust_exception_frame))
+		  PV_INDIRECT_CALL(irq,IRQ,adjust_exception_frame))
 
 #define USERGS_SYSRET64							\
 	PARA_SITE(PARA_PATCH(pv_cpu_ops, PV_CPU_usergs_sysret64),	\
 		  CLBR_NONE,						\
-		  jmp PARA_INDIRECT(pv_cpu_ops+PV_CPU_usergs_sysret64))
+		  jmp *PARA_INDIRECT(pv_cpu_ops+PV_CPU_usergs_sysret64))
+
+#define GET_CR0_INTO_RDI				\
+	PV_INDIRECT_CALL(cpu,CPU,read_cr0);		\
+	mov %rax,%rdi
+
+#define SET_RDI_INTO_CR0				\
+	PV_INDIRECT_CALL(cpu,CPU,write_cr0)
+
+#define GET_CR3_INTO_RDI				\
+	PV_INDIRECT_CALL(mmu,MMU,read_cr3);		\
+	mov %rax,%rdi
+
+#define SET_RDI_INTO_CR3				\
+	PV_INDIRECT_CALL(mmu,MMU,write_cr3)
+
 #endif	/* CONFIG_X86_32 */
 
 #endif /* __ASSEMBLY__ */

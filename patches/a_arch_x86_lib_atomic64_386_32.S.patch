diff --git a/arch/x86/lib/atomic64_386_32.S b/arch/x86/lib/atomic64_386_32.S
index 9b0ca8f..390af89 100644
--- a/arch/x86/lib/atomic64_386_32.S
+++ b/arch/x86/lib/atomic64_386_32.S
@@ -10,6 +10,7 @@
  */
 
 #include <linux/linkage.h>
+#include <asm/irq_vectors.h>
 #include <asm/alternative-asm.h>
 
 /* if you want SMP support, implement these with real spinlocks */
@@ -32,26 +33,34 @@ ENTRY(atomic64_##op##_386); \
 
 #define ENDP endp
 
-#define RET \
+#define RET(op) \
 	UNLOCK v; \
-	ret
+	pax_ret atomic64_##op##_386
 
-#define RET_ENDP \
-	RET; \
+#define RET_ENDP(op) \
+	RET(op); \
 	ENDP
 
 #define v %ecx
 BEGIN(read)
 	movl  (v), %eax
 	movl 4(v), %edx
-RET_ENDP
+RET_ENDP(read)
+BEGIN(read_unchecked)
+	movl  (v), %eax
+	movl 4(v), %edx
+RET_ENDP(read_unchecked)
 #undef v
 
 #define v %esi
 BEGIN(set)
 	movl %ebx,  (v)
 	movl %ecx, 4(v)
-RET_ENDP
+RET_ENDP(set)
+BEGIN(set_unchecked)
+	movl %ebx,  (v)
+	movl %ecx, 4(v)
+RET_ENDP(set_unchecked)
 #undef v
 
 #define v  %esi
@@ -60,30 +69,51 @@ BEGIN(xchg)
 	movl 4(v), %edx
 	movl %ebx,  (v)
 	movl %ecx, 4(v)
-RET_ENDP
+RET_ENDP(xchg)
 #undef v
 
 #define v %ecx
 BEGIN(add)
 	addl %eax,  (v)
 	adcl %edx, 4(v)
-RET_ENDP
+
+	PAX_REFCOUNT64_OVERFLOW (v)
+RET_ENDP(add)
+BEGIN(add_unchecked)
+	addl %eax,  (v)
+	adcl %edx, 4(v)
+RET_ENDP(add_unchecked)
 #undef v
 
 #define v %ecx
 BEGIN(add_return)
 	addl  (v), %eax
 	adcl 4(v), %edx
+
 	movl %eax,  (v)
 	movl %edx, 4(v)
-RET_ENDP
+
+	PAX_REFCOUNT64_OVERFLOW (v)
+RET_ENDP(add_return)
+BEGIN(add_return_unchecked)
+	addl  (v), %eax
+	adcl 4(v), %edx
+	movl %eax,  (v)
+	movl %edx, 4(v)
+RET_ENDP(add_return_unchecked)
 #undef v
 
 #define v %ecx
 BEGIN(sub)
 	subl %eax,  (v)
 	sbbl %edx, 4(v)
-RET_ENDP
+
+	PAX_REFCOUNT64_UNDERFLOW (v)
+RET_ENDP(sub)
+BEGIN(sub_unchecked)
+	subl %eax,  (v)
+	sbbl %edx, 4(v)
+RET_ENDP(sub_unchecked)
 #undef v
 
 #define v %ecx
@@ -93,16 +123,34 @@ BEGIN(sub_return)
 	sbbl $0, %edx
 	addl  (v), %eax
 	adcl 4(v), %edx
+
 	movl %eax,  (v)
 	movl %edx, 4(v)
-RET_ENDP
+
+	PAX_REFCOUNT64_UNDERFLOW (v)
+RET_ENDP(sub_return)
+BEGIN(sub_return_unchecked)
+	negl %edx
+	negl %eax
+	sbbl $0, %edx
+	addl  (v), %eax
+	adcl 4(v), %edx
+	movl %eax,  (v)
+	movl %edx, 4(v)
+RET_ENDP(sub_return_unchecked)
 #undef v
 
 #define v %esi
 BEGIN(inc)
 	addl $1,  (v)
 	adcl $0, 4(v)
-RET_ENDP
+
+	PAX_REFCOUNT64_OVERFLOW (v)
+RET_ENDP(inc)
+BEGIN(inc_unchecked)
+	addl $1,  (v)
+	adcl $0, 4(v)
+RET_ENDP(inc_unchecked)
 #undef v
 
 #define v %esi
@@ -111,16 +159,33 @@ BEGIN(inc_return)
 	movl 4(v), %edx
 	addl $1, %eax
 	adcl $0, %edx
+
 	movl %eax,  (v)
 	movl %edx, 4(v)
-RET_ENDP
+
+	PAX_REFCOUNT64_OVERFLOW (v)
+RET_ENDP(inc_return)
+BEGIN(inc_return_unchecked)
+	movl  (v), %eax
+	movl 4(v), %edx
+	addl $1, %eax
+	adcl $0, %edx
+	movl %eax,  (v)
+	movl %edx, 4(v)
+RET_ENDP(inc_return_unchecked)
 #undef v
 
 #define v %esi
 BEGIN(dec)
 	subl $1,  (v)
 	sbbl $0, 4(v)
-RET_ENDP
+
+	PAX_REFCOUNT64_UNDERFLOW (v)
+RET_ENDP(dec)
+BEGIN(dec_unchecked)
+	subl $1,  (v)
+	sbbl $0, 4(v)
+RET_ENDP(dec_unchecked)
 #undef v
 
 #define v %esi
@@ -129,9 +194,20 @@ BEGIN(dec_return)
 	movl 4(v), %edx
 	subl $1, %eax
 	sbbl $0, %edx
+
 	movl %eax,  (v)
 	movl %edx, 4(v)
-RET_ENDP
+
+	PAX_REFCOUNT64_UNDERFLOW (v)
+RET_ENDP(dec_return)
+BEGIN(dec_return_unchecked)
+	movl  (v), %eax
+	movl 4(v), %edx
+	subl $1, %eax
+	sbbl $0, %edx
+	movl %eax,  (v)
+	movl %edx, 4(v)
+RET_ENDP(dec_return_unchecked)
 #undef v
 
 #define v %esi
@@ -140,6 +216,9 @@ BEGIN(add_unless)
 	adcl %edx, %edi
 	addl  (v), %eax
 	adcl 4(v), %edx
+
+	PAX_REFCOUNT64_OVERFLOW (v)
+
 	cmpl %eax, %ecx
 	je 3f
 1:
@@ -147,7 +226,7 @@ BEGIN(add_unless)
 	movl %edx, 4(v)
 	movl $1, %eax
 2:
-	RET
+	RET(add_unless)
 3:
 	cmpl %edx, %edi
 	jne 1b
@@ -165,11 +244,14 @@ BEGIN(inc_not_zero)
 1:
 	addl $1, %eax
 	adcl $0, %edx
+
+	PAX_REFCOUNT64_OVERFLOW (v)
+
 	movl %eax,  (v)
 	movl %edx, 4(v)
 	movl $1, %eax
 2:
-	RET
+	RET(inc_not_zero)
 3:
 	testl %edx, %edx
 	jne 1b
@@ -183,9 +265,12 @@ BEGIN(dec_if_positive)
 	movl 4(v), %edx
 	subl $1, %eax
 	sbbl $0, %edx
+
+	PAX_REFCOUNT64_UNDERFLOW (v)
+
 	js 1f
 	movl %eax,  (v)
 	movl %edx, 4(v)
 1:
-RET_ENDP
+RET_ENDP(dec_if_positive)
 #undef v

diff --git a/arch/x86/crypto/crc32-pclmul_asm.S b/arch/x86/crypto/crc32-pclmul_asm.S
index f247304..d253bd1 100644
--- a/arch/x86/crypto/crc32-pclmul_asm.S
+++ b/arch/x86/crypto/crc32-pclmul_asm.S
@@ -39,6 +39,7 @@
 
 #include <linux/linkage.h>
 #include <asm/inst.h>
+#include <asm/alternative-asm.h>
 
 
 .align 16
@@ -102,6 +103,12 @@
  *	                     size_t len, uint crc32)
  */
 
+#ifndef __x86_64__
+__i686_get_pc_thunk_cx:
+	mov     (%esp),%ecx
+	ret
+#endif
+
 ENTRY(crc32_pclmul_le_16) /* buffer and buffer size are 16 bytes aligned */
 	movdqa  (BUF), %xmm1
 	movdqa  0x10(BUF), %xmm2
@@ -113,9 +120,8 @@ ENTRY(crc32_pclmul_le_16) /* buffer and buffer size are 16 bytes aligned */
 	add     $0x40, BUF
 #ifndef __x86_64__
 	/* This is for position independent code(-fPIC) support for 32bit */
-	call    delta
+	call    __i686_get_pc_thunk_cx
 delta:
-	pop     %ecx
 #endif
 	cmp     $0x40, LEN
 	jb      less_64
@@ -123,7 +129,7 @@ delta:
 #ifdef __x86_64__
 	movdqa .Lconstant_R2R1(%rip), CONSTANT
 #else
-	movdqa .Lconstant_R2R1 - delta(%ecx), CONSTANT
+	movdqa %cs:.Lconstant_R2R1 - delta (%ecx), CONSTANT
 #endif
 
 loop_64:/*  64 bytes Full cache line folding */
@@ -172,7 +178,7 @@ less_64:/*  Folding cache line into 128bit */
 #ifdef __x86_64__
 	movdqa  .Lconstant_R4R3(%rip), CONSTANT
 #else
-	movdqa  .Lconstant_R4R3 - delta(%ecx), CONSTANT
+	movdqa  %cs:.Lconstant_R4R3 - delta(%ecx), CONSTANT
 #endif
 	prefetchnta     (BUF)
 
@@ -220,8 +226,8 @@ fold_64:
 	movdqa  .Lconstant_R5(%rip), CONSTANT
 	movdqa  .Lconstant_mask32(%rip), %xmm3
 #else
-	movdqa  .Lconstant_R5 - delta(%ecx), CONSTANT
-	movdqa  .Lconstant_mask32 - delta(%ecx), %xmm3
+	movdqa  %cs:.Lconstant_R5 - delta(%ecx), CONSTANT
+	movdqa  %cs:.Lconstant_mask32 - delta(%ecx), %xmm3
 #endif
 	psrldq  $0x04, %xmm2
 	pand    %xmm3, %xmm1
@@ -232,7 +238,7 @@ fold_64:
 #ifdef __x86_64__
 	movdqa  .Lconstant_RUpoly(%rip), CONSTANT
 #else
-	movdqa  .Lconstant_RUpoly - delta(%ecx), CONSTANT
+	movdqa  %cs:.Lconstant_RUpoly - delta(%ecx), CONSTANT
 #endif
 	movdqa  %xmm1, %xmm2
 	pand    %xmm3, %xmm1
@@ -242,5 +248,5 @@ fold_64:
 	pxor    %xmm2, %xmm1
 	PEXTRD  0x01, %xmm1, %eax
 
-	ret
+	pax_ret crc32_pclmul_le_16
 ENDPROC(crc32_pclmul_le_16)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 154fd68..f95f804 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -2268,7 +2268,7 @@ void set_numabalancing_state(bool enabled)
 int sysctl_numa_balancing(struct ctl_table *table, int write,
 			 void __user *buffer, size_t *lenp, loff_t *ppos)
 {
-	struct ctl_table t;
+	ctl_table_no_const t;
 	int err;
 	int state = static_branch_likely(&sched_numa_balancing);
 
@@ -2343,7 +2343,7 @@ static void __init init_schedstats(void)
 int sysctl_schedstats(struct ctl_table *table, int write,
 			 void __user *buffer, size_t *lenp, loff_t *ppos)
 {
-	struct ctl_table t;
+	ctl_table_no_const t;
 	int err;
 	int state = static_branch_likely(&sched_schedstats);
 
@@ -2797,7 +2797,7 @@ static struct rq *finish_task_switch(struct task_struct *prev)
 /* rq->lock is NOT held, but preemption is disabled */
 static void __balance_callback(struct rq *rq)
 {
-	struct callback_head *head, *next;
+	struct balance_callback *head, *next;
 	void (*func)(struct rq *rq);
 	unsigned long flags;
 
@@ -2805,7 +2805,7 @@ static void __balance_callback(struct rq *rq)
 	head = rq->balance_callback;
 	rq->balance_callback = NULL;
 	while (head) {
-		func = (void (*)(struct rq *))head->func;
+		func = head->func;
 		next = head->next;
 		head->next = NULL;
 		head = next;
@@ -3795,6 +3795,8 @@ int can_nice(const struct task_struct *p, const int nice)
 	/* convert nice value [19,-20] to rlimit style value [1,40] */
 	int nice_rlim = nice_to_rlimit(nice);
 
+	gr_learn_resource(p, RLIMIT_NICE, nice_rlim, 1);
+
 	return (nice_rlim <= task_rlimit(p, RLIMIT_NICE) ||
 		capable(CAP_SYS_NICE));
 }
@@ -3821,7 +3823,8 @@ SYSCALL_DEFINE1(nice, int, increment)
 	nice = task_nice(current) + increment;
 
 	nice = clamp_val(nice, MIN_NICE, MAX_NICE);
-	if (increment < 0 && !can_nice(current, nice))
+	if (increment < 0 && (!can_nice(current, nice) ||
+			      gr_handle_chroot_nice()))
 		return -EPERM;
 
 	retval = security_task_setnice(current, nice);
@@ -4131,6 +4134,7 @@ static int __sched_setscheduler(struct task_struct *p,
 			if (policy != p->policy && !rlim_rtprio)
 				return -EPERM;
 
+			gr_learn_resource(p, RLIMIT_RTPRIO, attr->sched_priority, 1);
 			/* can't increase priority */
 			if (attr->sched_priority > p->rt_priority &&
 			    attr->sched_priority > rlim_rtprio)
@@ -7591,6 +7595,30 @@ void __init sched_init(void)
 	for_each_possible_cpu(i) {
 		struct rq *rq;
 
+#if defined(CONFIG_GRKERNSEC_KSTACKOVERFLOW) && defined(CONFIG_X86_64)
+		struct page *newstack_page = alloc_pages_node(cpu_to_node(i), GFP_KERNEL|__GFP_NOTRACK|__GFP_ZERO, IRQ_STACK_ORDER);
+		void *newstack_lowmem = NULL;
+		void *newstack;
+		struct page *pages[IRQ_STACK_SIZE / PAGE_SIZE];
+		unsigned int x;
+
+		if (newstack_page)
+			newstack_lowmem = page_address(newstack_page);
+
+		if (newstack_lowmem == NULL)
+			panic("grsec: Unable to allocate irq stack");
+
+		for (x = 0; x < IRQ_STACK_SIZE / PAGE_SIZE; x++)
+			pages[x] = virt_to_page(newstack_lowmem + (x * PAGE_SIZE));
+
+		newstack = vmap(pages, IRQ_STACK_SIZE / PAGE_SIZE, VM_IOREMAP, PAGE_KERNEL);
+		if (newstack == NULL)
+			panic("grsec: Unable to vmap irq stack");
+		populate_stack(newstack, IRQ_STACK_SIZE);
+		per_cpu(irq_stack_ptr_lowmem, i) = newstack_lowmem + IRQ_STACK_SIZE - 64;
+		per_cpu(irq_stack_ptr, i) = newstack + IRQ_STACK_SIZE - 64;
+#endif
+
 		rq = cpu_rq(i);
 		raw_spin_lock_init(&rq->lock);
 		rq->nr_running = 0;
@@ -7712,7 +7740,7 @@ void __might_sleep(const char *file, int line, int preempt_offset)
 	 */
 	WARN_ONCE(current->state != TASK_RUNNING && current->task_state_change,
 			"do not call blocking ops when !TASK_RUNNING; "
-			"state=%lx set at [<%p>] %pS\n",
+			"state=%lx set at [<%p>] %pA\n",
 			current->state,
 			(void *)current->task_state_change,
 			(void *)current->task_state_change);

diff --git a/fs/dcache.c b/fs/dcache.c
index 4485a48..2cdeaf6 100644
--- a/fs/dcache.c
+++ b/fs/dcache.c
@@ -321,8 +321,9 @@ static void dentry_free(struct dentry *dentry)
  * d_iput() operation if defined.
  */
 static void dentry_unlink_inode(struct dentry * dentry)
-	__releases(dentry->d_lock)
-	__releases(dentry->d_inode->i_lock)
+	__releases(&dentry->d_lock)
+	__releases(&dentry->d_inode->i_lock);
+static void dentry_unlink_inode(struct dentry * dentry)
 {
 	struct inode *inode = dentry->d_inode;
 	bool hashed = !d_unhashed(dentry);
@@ -559,7 +560,8 @@ static void __dentry_kill(struct dentry *dentry)
  * Returns dentry requiring refcount drop, or NULL if we're done.
  */
 static struct dentry *dentry_kill(struct dentry *dentry)
-	__releases(dentry->d_lock)
+	__releases(&dentry->d_lock);
+static struct dentry *dentry_kill(struct dentry *dentry)
 {
 	struct inode *inode = dentry->d_inode;
 	struct dentry *parent = NULL;
@@ -589,7 +591,7 @@ static inline struct dentry *lock_parent(struct dentry *dentry)
 	struct dentry *parent = dentry->d_parent;
 	if (IS_ROOT(dentry))
 		return NULL;
-	if (unlikely(dentry->d_lockref.count < 0))
+	if (unlikely(__lockref_read(&dentry->d_lockref) < 0))
 		return NULL;
 	if (likely(spin_trylock(&parent->d_lock)))
 		return parent;
@@ -651,8 +653,8 @@ static inline bool fast_dput(struct dentry *dentry)
 	 */
 	if (unlikely(ret < 0)) {
 		spin_lock(&dentry->d_lock);
-		if (dentry->d_lockref.count > 1) {
-			dentry->d_lockref.count--;
+		if (__lockref_read(&dentry->d_lockref) > 1) {
+			__lockref_dec(&dentry->d_lockref);
 			spin_unlock(&dentry->d_lock);
 			return 1;
 		}
@@ -707,7 +709,7 @@ static inline bool fast_dput(struct dentry *dentry)
 	 * else could have killed it and marked it dead. Either way, we
 	 * don't need to do anything else.
 	 */
-	if (dentry->d_lockref.count) {
+	if (__lockref_read(&dentry->d_lockref)) {
 		spin_unlock(&dentry->d_lock);
 		return 1;
 	}
@@ -717,7 +719,7 @@ static inline bool fast_dput(struct dentry *dentry)
 	 * lock, and we just tested that it was zero, so we can just
 	 * set it to 1.
 	 */
-	dentry->d_lockref.count = 1;
+	__lockref_set(&dentry->d_lockref, 1);
 	return 0;
 }
 
@@ -754,8 +756,6 @@ void dput(struct dentry *dentry)
 		return;
 
 repeat:
-	might_sleep();
-
 	rcu_read_lock();
 	if (likely(fast_dput(dentry))) {
 		rcu_read_unlock();
@@ -783,7 +783,7 @@ void dput(struct dentry *dentry)
 		dentry->d_flags |= DCACHE_REFERENCED;
 	dentry_lru_add(dentry);
 
-	dentry->d_lockref.count--;
+	__lockref_dec(&dentry->d_lockref);
 	spin_unlock(&dentry->d_lock);
 	return;
 
@@ -800,7 +800,7 @@ EXPORT_SYMBOL(dput);
 /* This must be called with d_lock held */
 static inline void __dget_dlock(struct dentry *dentry)
 {
-	dentry->d_lockref.count++;
+	__lockref_inc(&dentry->d_lockref);
 }
 
 static inline void __dget(struct dentry *dentry)
@@ -841,8 +841,8 @@ struct dentry *dget_parent(struct dentry *dentry)
 		goto repeat;
 	}
 	rcu_read_unlock();
-	BUG_ON(!ret->d_lockref.count);
-	ret->d_lockref.count++;
+	BUG_ON(!__lockref_read(&ret->d_lockref));
+	__lockref_inc(&ret->d_lockref);
 	spin_unlock(&ret->d_lock);
 	return ret;
 }
@@ -920,9 +920,9 @@ void d_prune_aliases(struct inode *inode)
 	spin_lock(&inode->i_lock);
 	hlist_for_each_entry(dentry, &inode->i_dentry, d_u.d_alias) {
 		spin_lock(&dentry->d_lock);
-		if (!dentry->d_lockref.count) {
+		if (!__lockref_read(&dentry->d_lockref)) {
 			struct dentry *parent = lock_parent(dentry);
-			if (likely(!dentry->d_lockref.count)) {
+			if (likely(!__lockref_read(&dentry->d_lockref))) {
 				__dentry_kill(dentry);
 				dput(parent);
 				goto restart;
@@ -957,7 +957,7 @@ static void shrink_dentry_list(struct list_head *list)
 		 * We found an inuse dentry which was not removed from
 		 * the LRU because of laziness during lookup. Do not free it.
 		 */
-		if (dentry->d_lockref.count > 0) {
+		if (__lockref_read(&dentry->d_lockref) > 0) {
 			spin_unlock(&dentry->d_lock);
 			if (parent)
 				spin_unlock(&parent->d_lock);
@@ -995,8 +995,8 @@ static void shrink_dentry_list(struct list_head *list)
 		dentry = parent;
 		while (dentry && !lockref_put_or_lock(&dentry->d_lockref)) {
 			parent = lock_parent(dentry);
-			if (dentry->d_lockref.count != 1) {
-				dentry->d_lockref.count--;
+			if (__lockref_read(&dentry->d_lockref) != 1) {
+				__lockref_inc(&dentry->d_lockref);
 				spin_unlock(&dentry->d_lock);
 				if (parent)
 					spin_unlock(&parent->d_lock);
@@ -1036,7 +1036,7 @@ static enum lru_status dentry_lru_isolate(struct list_head *item,
 	 * counts, just remove them from the LRU. Otherwise give them
 	 * another pass through the LRU.
 	 */
-	if (dentry->d_lockref.count) {
+	if (__lockref_read(&dentry->d_lockref)) {
 		d_lru_isolate(lru, dentry);
 		spin_unlock(&dentry->d_lock);
 		return LRU_REMOVED;
@@ -1376,7 +1376,7 @@ static enum d_walk_ret select_collect(void *_data, struct dentry *dentry)
 	} else {
 		if (dentry->d_flags & DCACHE_LRU_LIST)
 			d_lru_del(dentry);
-		if (!dentry->d_lockref.count) {
+		if (!__lockref_read(&dentry->d_lockref)) {
 			d_shrink_add(dentry, &data->dispose);
 			data->found++;
 		}
@@ -1424,7 +1424,7 @@ static enum d_walk_ret umount_check(void *_data, struct dentry *dentry)
 		return D_WALK_CONTINUE;
 
 	/* root with refcount 1 is fine */
-	if (dentry == _data && dentry->d_lockref.count == 1)
+	if (dentry == _data && __lockref_read(&dentry->d_lockref) == 1)
 		return D_WALK_CONTINUE;
 
 	printk(KERN_ERR "BUG: Dentry %p{i=%lx,n=%pd} "
@@ -1433,7 +1433,7 @@ static enum d_walk_ret umount_check(void *_data, struct dentry *dentry)
 		       dentry->d_inode ?
 		       dentry->d_inode->i_ino : 0UL,
 		       dentry,
-		       dentry->d_lockref.count,
+		       __lockref_read(&dentry->d_lockref),
 		       dentry->d_sb->s_type->name,
 		       dentry->d_sb->s_id);
 	WARN_ON(1);
@@ -1579,7 +1579,7 @@ struct dentry *__d_alloc(struct super_block *sb, const struct qstr *name)
 		dname = dentry->d_iname;
 	} else if (name->len > DNAME_INLINE_LEN-1) {
 		size_t size = offsetof(struct external_name, name[1]);
-		struct external_name *p = kmalloc(size + name->len,
+		struct external_name *p = kmalloc(round_up(size + name->len, sizeof(unsigned long)),
 						  GFP_KERNEL_ACCOUNT);
 		if (!p) {
 			kmem_cache_free(dentry_cache, dentry); 
@@ -1603,7 +1603,7 @@ struct dentry *__d_alloc(struct super_block *sb, const struct qstr *name)
 	smp_wmb();
 	dentry->d_name.name = dname;
 
-	dentry->d_lockref.count = 1;
+	__lockref_set(&dentry->d_lockref, 1);
 	dentry->d_flags = 0;
 	spin_lock_init(&dentry->d_lock);
 	seqcount_init(&dentry->d_seq);
@@ -1612,6 +1612,9 @@ struct dentry *__d_alloc(struct super_block *sb, const struct qstr *name)
 	dentry->d_sb = sb;
 	dentry->d_op = NULL;
 	dentry->d_fsdata = NULL;
+#ifdef CONFIG_GRKERNSEC_CHROOT_RENAME
+	atomic_set(&dentry->chroot_refcnt, 0);
+#endif
 	INIT_HLIST_BL_NODE(&dentry->d_hash);
 	INIT_LIST_HEAD(&dentry->d_lru);
 	INIT_LIST_HEAD(&dentry->d_subdirs);
@@ -2253,7 +2256,7 @@ struct dentry *__d_lookup(const struct dentry *parent, const struct qstr *name)
 		if (!d_same_name(dentry, parent, name))
 			goto next;
 
-		dentry->d_lockref.count++;
+		__lockref_inc(&dentry->d_lockref);
 		found = dentry;
 		spin_unlock(&dentry->d_lock);
 		break;
@@ -2321,7 +2324,7 @@ void d_delete(struct dentry * dentry)
 	spin_lock(&dentry->d_lock);
 	inode = dentry->d_inode;
 	isdir = S_ISDIR(inode->i_mode);
-	if (dentry->d_lockref.count == 1) {
+	if (__lockref_read(&dentry->d_lockref) == 1) {
 		if (!spin_trylock(&inode->i_lock)) {
 			spin_unlock(&dentry->d_lock);
 			cpu_relax();
@@ -3503,7 +3506,7 @@ static enum d_walk_ret d_genocide_kill(void *data, struct dentry *dentry)
 
 		if (!(dentry->d_flags & DCACHE_GENOCIDE)) {
 			dentry->d_flags |= DCACHE_GENOCIDE;
-			dentry->d_lockref.count--;
+			__lockref_dec(&dentry->d_lockref);
 		}
 	}
 	return D_WALK_CONTINUE;
@@ -3574,8 +3577,8 @@ static void __init dcache_init(void)
 	 * but it is probably not worth it because of the cache nature
 	 * of the dcache. 
 	 */
-	dentry_cache = KMEM_CACHE(dentry,
-		SLAB_RECLAIM_ACCOUNT|SLAB_PANIC|SLAB_MEM_SPREAD|SLAB_ACCOUNT);
+	dentry_cache = KMEM_CACHE_USERCOPY(dentry,
+		SLAB_RECLAIM_ACCOUNT|SLAB_PANIC|SLAB_MEM_SPREAD|SLAB_ACCOUNT, d_iname);
 
 	/* Hash may have been set up in dcache_init_early */
 	if (!hashdist)
@@ -3610,8 +3613,9 @@ void __init vfs_caches_init_early(void)
 
 void __init vfs_caches_init(void)
 {
-	names_cachep = kmem_cache_create("names_cache", PATH_MAX, 0,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
+	names_cachep = kmem_cache_create_usercopy("names_cache", PATH_MAX, 0,
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC| SLAB_NO_SANITIZE,
+			0, PATH_MAX, NULL);
 
 	dcache_init();
 	inode_init();
